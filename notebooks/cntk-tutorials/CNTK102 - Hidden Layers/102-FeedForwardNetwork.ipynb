{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retroactive changes\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://github.com/SpaceAntelope/IfCntk/blob/master/notebooks/cntk-tutorials/Preparing%20workspace.ipynb\">Prepare workspace notebook</a> (<a href=\"https://areslazarus.com/archive/net-deep-learning-stack-cntk-101-logistic-regression/\">blog</a>) updated to include <a href=\"http://fsprojects.github.io/FSharp.Control.AsyncSeq/\">FSharp.Control.AsyncSeq</a> dependency.</li>\n",
    "    <li>Undescore_separated variable names now indicate a global variable original to the Python tutorial</li>\n",
    "    <li>Comments starting with <code>// #</code> indicate an unchanged comment from the original python code.</li>    \n",
    "</ol>\n",
    "    \n",
    "I'm also moving all reuseable chart related helper functions into their own script file at <strong>NbVisual.fsx</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the workspace for CNTK in jupyter, as always\n",
    "\n",
    "If referencing CNTK fails, make sure you have followed the instructions in my [Preparing Workspace.ipynb](https://github.com/SpaceAntelope/IfCntk/tree/master/notebooks/cntk-tutorials/Preparing%20workspace.ipynb) notebook. The current notebook assumes that all necessary CNTK nuget DLLs have been copied to a folder named **bin** in the parent path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load \"Paket.fsx\"\n",
    "\n",
    "Paket.Dependencies.Install \"\"\"\n",
    "framework: netstandard2.0\n",
    "generate_load_scripts: true\n",
    "storage: none\n",
    "source https://nuget.org/api/v2\n",
    "nuget CNTK.CPUOnly\n",
    "nuget MathNet.Numerics\n",
    "nuget MathNet.Numerics.FSharp\n",
    "nuget FSharp.Control.AsyncSeq\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#r \"netstandard\"\n",
    "#r \"../bin\\Cntk.Core.Managed-2.6.dll\"\n",
    "#load \"../CNTK102 - Hidden Layers/.paket/load/main.group.fsx\"\n",
    " \n",
    "open System\n",
    "open System.IO\n",
    "\n",
    "Environment.GetEnvironmentVariable(\"PATH\")\n",
    "|> fun path -> sprintf \"%s%c%s\" path (Path.PathSeparator) (Path.GetFullPath(\"../bin\"))\n",
    "|> fun path -> Environment.SetEnvironmentVariable(\"PATH\", path)\n",
    "\n",
    "open CNTK\n",
    "DeviceDescriptor.UseDefaultDevice().Type\n",
    "|> printfn \"Congratulations, you are using CNTK for: %A\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's reference any helper functions from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Setup display support\n",
    "#load \"AsyncDisplay.fsx\"\n",
    "#load \"XPlot.Plotly.fsx\"\n",
    "\n",
    "// Set globals that affect CNTK code in the helpers\n",
    "let device = CNTK.DeviceDescriptor.CPUDevice\n",
    "let dataType = CNTK.DataType.Float\n",
    "let initialization = CNTKLib.GlorotUniformInitializer(1.0)\n",
    "\n",
    "#r \"IfSharp.Kernel\"\n",
    "#load \"../fsx/NBHelpers.fsx\"\n",
    "#load \"../fsx/MiscellaneousHelpers.fsx\"\n",
    "#load \"../fsx/NbVisual.fsx\"\n",
    "#load \"../fsx/CntkHelpers.fsx\"\n",
    "\n",
    "open NBHelpers\n",
    "open MiscellaneousHelpers\n",
    "open NbVisual\n",
    "open CntkHelpers\n",
    "open XPlot.Plotly\n",
    "open MathNet.Numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">This section needs to be revisited since moving notebooks to their own folders means the <code>main.group.fsx</code> overlap can be avoided entirely. However the path change made <code>IfSharp.Kernel</code> unavailable from loaded scripts unless they include a full path reference to the local IfSharp folder dll. My current workaround is to reference it in-notebook (see above) which makes commiting a local path unnecessary.\n",
    "\n",
    "Occasionally bootstrap isn't available, still looking into why.</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <p>Since we are using the <code>storage: none</code> option in paket (see <a href=\"\">Preparing the workspace</a> notebook) attempting to load <strong>AsyncDisplay.fsx</strong> as is produces errors. This is because the script looks to resolve its dependencies within the IfSharp.exe directory tree.</p></div>\n",
    "\n",
    "<p>Running <strong>AsyncDisplay.Paket.fsx</strong> just like in the <a href=\"https://github.com/fsprojects/IfSharp/blob/master/FSharp_Jupyter_Notebooks.ipynb\">IfSharp feature notebook</a> would make the library available there, but it has two important drawbacks:\n",
    "    <ol type=\"i\"> \n",
    "        <li>You are locked into using a particular version of the library</li>\n",
    "        <li><i>/IfSharp/.paket/load/main.group.fsx</i> now overrides the scope of this notebook, meaning we lose access to the libraries in the global .nuget folder unless we delete that particular script.</li>\n",
    "    </ol>\n",
    "</p>\n",
    "<p>A simple solution is to just remove the first <i>#load</i> directive from <strong>AsyncDisplay.fsx</strong> to prevent it from looking for <strong>FSharp.Control.AsyncSeq</strong> in the IfSharp tree.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">For more info on setting the device descriptor see the <a href=\"https://github.com/SpaceAntelope/IfCntk/notebooks/cntk-tutorials/101-LogReg-CPUOnly.ipynb\">previous notebook</a> under the heading <strong><a href=\"https://github.com/SpaceAntelope/IfCntk/blob/master/notebooks/cntk-tutorials/101-LogReg-CPUOnly.ipynb#Global-variables\">Global variables</a></strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 102: Feed Forward Network with Simulated Data\n",
    "\n",
    "[Link to original python notebook](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_102_FeedForward.ipynb)\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML theory aside, the gist of this tutorial is:\n",
    "<ol type=\"i\">\n",
    "    <li>Learn to connect multiple layers like the one in CNTK 101 in order to form a deep learning model and\n",
    "    <li>Formalize a machine learning pipeline from data to model.\n",
    "</ol>        \n",
    "\n",
    "The pipeline consists of Data access, Data transformation, Model creation, Training and Evaluating. This is explictly modelled in [ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet).\n",
    "\n",
    "Apart from that, CNTK-102 is just CNTK-101 reloaded, with much of the same code reappearing. So in order to keep things interesting I embelished a bit: \n",
    "<ol type=\"i\" start=\"3\">\n",
    "    <li> I added a section on using the freshly introduced hidden layer architecture to perform classification on a non linearly separable version of the data generator introduced in 101.</li>\n",
    "    <li> I added an example of using <strong><a href=\"http://fsprojects.github.io/FSharp.Control.AsyncSeq/\">FSharp.Control.AsyncSeq</a></strong> and IfSharp's <strong>AsyncDisplay.fsx</strong> to display training data on the notebook while training is ongoing.\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// # Figure 1\n",
    " ImageUrl \"https://www.cntk.ai/jup/cancer_data_plot.jpg\" 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Figure 2\n",
    "ImageUrl \"https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif\" 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Ensure we always get the same amount of randomness\n",
    "MiscellaneousHelpers.seed <- 42\n",
    "\n",
    "// # Define the data dimensions\n",
    "let input_dim = 2\n",
    "let num_output_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Create the input variables denoting the features and the label data. Note: the input \n",
    "// # does not need additional info on number of observations (Samples) since CNTK first create only \n",
    "// # the network topology first \n",
    "let mysamplesize = 64\n",
    "let features, labels = generateRandomDataSample mysamplesize input_dim num_output_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleScatterPlot \"Scaled age (in yrs)\" \"Tumor size (in cm)\" features labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageUrl \"http://cntk.ai/jup/feedforward_network.jpg\" 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let num_hidden_layers = 2\n",
    "let hidden_layers_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # The input variable (representing 1 observation, in our example of age and size) x, which \n",
    "// # in this case has a dimension of 2. \n",
    "// #\n",
    "// # The label variable has a dimensionality equal to the number of output classes in our case 2. \n",
    "\n",
    "let input = Variable.InputVariable(shape [|input_dim|], dataType, \"Features\")\n",
    "let label = Variable.InputVariable(shape [|num_output_classes|], dataType, \"Labels\")\n",
    "let initialization = CNTKLib.GlorotUniformInitializer(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">Unlike the python API, weight initialization is not automatic in the managed API</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"z_1 = W·x+b\" |> Util.Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In C# Variables, Functions and Parameters types are mostly interchangeable when used as function arguments. These tiny helpers will help make the explicit conversion from CNTK.Function to CNTK.Variable easier to integrate in F# pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Convert Function to Variable\n",
    "/// <remarks> CNTK helper </remarks>\n",
    "let inline Var (x : CNTK.Function) = new Variable(x)\n",
    "\n",
    "/// Convert Variable to Function\n",
    "/// <remarks> CNTK helper </remarks>\n",
    "let inline Fun (x : CNTK.Variable) = x.ToFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Create a new linear layer in the W·x+b pattern\n",
    "/// <remarks> CNTK helper </helper>\n",
    "let linearLayer (inputVar : Variable) outputDim =\n",
    "    let inputDim = inputVar.Shape.[0] \n",
    "    \n",
    "    // Note that unlike the python example, the dimensionality of the output\n",
    "    // goes first in the parameter declaration, otherwise the connection \n",
    "    // cannot be propagated.\n",
    "    let weightParam = new Parameter(shape [outputDim; inputDim], dataType, initialization, device, \"Weights\")\n",
    "    let biasParam = new Parameter(shape [outputDim], dataType, 0.0, device, \"Bias\")    \n",
    "    \n",
    "    let dotProduct = CNTKLib.Times(weightParam, inputVar, \"Weighted input\")\n",
    "    CNTKLib.Plus(Var dotProduct, biasParam, \"Layer\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Create a new linear layer and fully connect it to \n",
    "/// an existing one through a specified differentiable function\n",
    "/// <remarks> CNTK helper </helper>\n",
    "let denseLayer (nonlinearity: Variable -> Function) inputVar outputDim  =\n",
    "    linearLayer inputVar outputDim\n",
    "    |> Var |> nonlinearity |> Var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed the order parameter order of the **denseLayer** generator function by putting the activation function parameter first, in order to facilitate composition (it will become obvious how very shortly). Also, since the purpose of **denseLayer** is to connect two layers together, and the existing layer is presented as a <code>CNTK.Variable</code>, it seems prudent to preemptively convert the result from Function to Variable as well.\n",
    "\n",
    "#### Just F# problems\n",
    "It's a matter of time before all this back and forth converting from Function to Variable and back gets tiring, but for now let's just persevere. If you're not inclined to, I urge you to check Mathias Brandewinder's [CNTK.FSharp](https://github.com/mathias-brandewinder/CNTK.FSharp) project, where he presents an elegant abstraction in the form of a [*Tensor* discriminated union](https://github.com/mathias-brandewinder/CNTK.FSharp/blob/master/CNTK.FSharp/Core.fs) that encapsulates Functions and Variables in a single type and a lot more on top.\n",
    "\n",
    "Here's is the dense layer creation function according to the logic of the original in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Define a multilayer feedforward classification model\n",
    "let fullyConnectedClassifierNet inputVar numOutputClasses hiddenLayerDim numHiddenLayers nonlinearity =\n",
    "    let mutable h = denseLayer nonlinearity inputVar hiddenLayerDim\n",
    "    for i in 1..numHiddenLayers do\n",
    "        h <- denseLayer nonlinearity h hiddenLayerDim \n",
    "    \n",
    "    // Note that we don't feed the output layer through \n",
    "    // the selected nonlinearity/activation function    \n",
    "    linearLayer h numOutputClasses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Create the fully connected classifier\n",
    "let z = fullyConnectedClassifierNet input num_output_classes hidden_layers_dim num_hidden_layers (CNTKLib.Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get idiomatic\n",
    "Sadly there seem to be no managed helpers to facilitate dense layer creation. \n",
    "\n",
    "So instead, here's a more functional version of our linear layer composing function, without mutable variables or for loops, and with the added bonus of enabling you to arbitrarily set the number and dimension of hidden layers in a single parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Fully connected linear layer composition function\n",
    "/// <remarks> CNTK helper </remarks>\n",
    "let fullyConnectedClassifierNet' inputVar (hiddenLayerDims: int seq) numOutputClasses nonlinearity =\n",
    "    (inputVar, hiddenLayerDims) \n",
    "    ||> Seq.fold (denseLayer nonlinearity)\n",
    "    |> fun model -> linearLayer model numOutputClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, passing an empty sequence instead of a list of hidden layer dimensions produces a linear model, just like if you had run <code>linearLayer</code> with the same params.\n",
    "\n",
    "And here's how we would use this to produce a model identical to <code>z</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let z' = \n",
    "    fullyConnectedClassifierNet' \n",
    "        input [hidden_layers_dim;hidden_layers_dim]  \n",
    "        num_output_classes (CNTKLib.Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"p = softmax(z_{final~layer})\" |> Util.Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"H(p)=-\\sum_{j=1}^C y_j log(p_j)\" |> Util.Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let loss = CNTKLib.CrossEntropyWithSoftmax(Var z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let eval_error = CNTKLib.ClassificationError(Var z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Instantiate the trainer object ot drive the model training\n",
    "let learning_rate = 0.05\n",
    "let lr_schedule = new CNTK.TrainingParameterScheduleDouble(learning_rate, uint32 CNTK.DataUnit.Minibatch)\n",
    "let learner = CNTKLib.SGDLearner(z.Parameters() |> ParVec, lr_schedule)\n",
    "let trainer = CNTK.Trainer.CreateTrainer(z, loss, eval_error, ResizeArray<CNTK.Learner>([learner]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions for reporting training progress first seen in the previous notebook have already been referenced with <code>open MiscellaneousHelpers</code> and can also be found [here](https://github.com/SpaceAntelope/IfCntk/blob/master/notebooks/cntk-tutorials/fsx/MiscellaneousHelpers.fsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Initialize the parameters for the trainer\n",
    "let minibatch_size = 25\n",
    "let num_samples = 20000\n",
    "let num_minibatches_to_train = num_samples / minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Run the trainer and perform model training\n",
    "let training_progress_output_freq = 20\n",
    "\n",
    "let plotdata = {\n",
    "    BatchSize = ResizeArray<int>()\n",
    "    Loss = ResizeArray<float>()\n",
    "    Error = ResizeArray<float>()\n",
    "}\n",
    "\n",
    "for i in [0..num_minibatches_to_train] do\n",
    "    let features,labels =\n",
    "        generateRandomDataSample minibatch_size input_dim num_output_classes\n",
    "        |> fun (x,y) -> matrixToBatch x, matrixToBatch y\n",
    "    \n",
    "    // # Specify the input variables mapping in the model to actual minibatch data for training\n",
    "    let trainingBatch = [(input, features);(label, labels)] |> dict\n",
    "    let status = trainer.TrainMinibatch(trainingBatch, true, device)\n",
    "    \n",
    "    // log training data\n",
    "    match (printTrainingProgress trainer i training_progress_output_freq true) with\n",
    "    | Some (i,loss,eval) ->         \n",
    "        plotdata.BatchSize.Add <| i\n",
    "        plotdata.Loss.Add <| loss\n",
    "        plotdata.Error.Add <| eval\n",
    "    | None -> ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Compute the moving average loss to smooth out the noise in SGD\n",
    "trainingResultPlotSmoothed plotdata |> Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation / testing\n",
    "This time, instead of reproducing the identical functionality of the python tutorial let's save our future selves a bit of time and create a reusable version of the evaluation/testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open MathNet.Numerics.LinearAlgebra\n",
    "\n",
    "/// Evaluation of a Matrix dataset for a trained model\n",
    "/// <remarks> CNTK helper </remarks>\n",
    "let testMinibatch (trainer: CNTK.Trainer) (features: Matrix<float32>) (labels: Matrix<float32>) =\n",
    "    let x,y = matrixToBatch features, matrixToBatch labels\n",
    "    \n",
    "    // It should be interesting to see if this convention\n",
    "    // will hold for any other topography     \n",
    "    let input = trainer.Model().Arguments |> Seq.head\n",
    "    let label = trainer.LossFunction().Arguments |> Seq.last\n",
    "    \n",
    "    let testBatch =\n",
    "        [ (input, x);(label, y) ]\n",
    "        |> dict\n",
    "        |> AsUnorderedMapVariableValue\n",
    "    \n",
    "    trainer.TestMinibatch(testBatch , device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Generate new data\n",
    "let test_minibatch_size = 25\n",
    "let x_test,y_test = generateRandomDataSample test_minibatch_size input_dim num_output_classes\n",
    "\n",
    "testMinibatch trainer x_test y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # Figure 4\n",
    "ImageUrl \"http://cntk.ai/jup/feedforward_network.jpg\" 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let out = CNTKLib.Softmax(Var z)\n",
    "let inputMap = [input, matrixToBatch x_test] |> dict\n",
    "let outputMap = [(out.Output, null)] |> dataMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "let predicted_label_probs = out.Evaluate(inputMap, outputMap, device)\n",
    "\n",
    "/// Get index of maximum value\n",
    "/// <remarks> Helper function </remarks>\n",
    "let argMax<'T when 'T : comparison and 'T : equality>(source: 'T seq) = \n",
    "    let max = source |> Seq.max \n",
    "    Seq.findIndex ((=)max) source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let result = outputMap.[out.Output].GetDenseData<float32>(out.Output)\n",
    "\n",
    "y_test |> Matrix.toRowArrays |> Array.map argMax |> printfn \"Label    : %A\"    \n",
    "result |> Seq.map argMax |> Array.ofSeq |> printfn \"Predicted: %A\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> Some times when trying to evaluate <code>result</code> in a shell an exception of type <code>Expression evaluation failed: Value::CopyFrom is currently unsupported for PackedValue objects</code> occurs. The solution is to take it from the top, and reset the model and the input/output maps. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelSoftmaxOutputHeatmap \"Scaled age (in yrs)\" \"Tumor size (in cm)\" [|1. .. 0.1 .. 10.|] z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span class=\"label label-success\">Extra!</span> Non linear separation example\n",
    "\n",
    "Seems a shame to go through all this trouble to build a hidden layer topography and not even try it on a more challenging example.\n",
    "\n",
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open MathNet.Numerics.LinearAlgebra\n",
    "\n",
    "// We achieve non linear separation by stealthily adding another output class,\n",
    "// that we then assign to the first class, thus encircling the rest of the data.\n",
    "let generateRandomNonlinearlySeparableDataSample sampleCount featureCount labelCount =     \n",
    "    let x,y = generateRandomDataSample sampleCount featureCount (labelCount+1)\n",
    "    let y' = \n",
    "        y \n",
    "        |> Matrix.toRowArrays \n",
    "        |> Array.map(\n",
    "            fun line -> \n",
    "                if line.[labelCount] = 1.f \n",
    "                then line.[0] <- 1.f\n",
    "                \n",
    "                line.[0..labelCount-1])\n",
    "        |> matrix\n",
    "    \n",
    "    x,y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><p>If you are using the version of MathNet linked in the <a href=\"https://www.nuget.org/packages/FsLab/\">FsLab</a> nuget package as of the time of this writing you will get a conversion error when creating the new matrix from the transformed label data.</p>\n",
    "    <p></p>\n",
    "<blockquote>Type mismatch. Expecting a\n",
    "    'float32 [][] -> 'a'    \n",
    "but given a\n",
    "    'int list list -> Matrix<int>'    \n",
    "    The type 'seq<float32 []>' does not match the type 'int list list'</blockquote>\n",
    "    \n",
    "<p>To resolve this make sure your paket dependencies point the latest version of <strong>MathNet.Numerics</strong> and <strong>MathNet.Numerics.FSharp</strong> </p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Adding a new class and then assigning it to one of the previous classes after the fact has the side effect of creating and imbalanced dataset, since we now have a class with twice as many samples as any of the others.</p><p>In case of a two-class dataset this is the most pronounced, since samples from the two classes will be produced at a 2:1 rate relative to each other. This makes convergence harder than it has to be and also produces misleading evaluation results since for instance always predicting class 1 gives 67% success.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let rnd = new Random()\n",
    "let shuffle = Seq.sortBy (fun _ -> rnd.Next())\n",
    "\n",
    "// This slightly awkward function truncates the overpopulated class to match\n",
    "// the size of the others, and makes sure the selected subset is randomly \n",
    "// distributed between the two clusters (i.e. the original doubled class\n",
    "// and the spurious additional set)\n",
    "let stratifiedSampling (features: Matrix<float32>) (labels: Matrix<float32>) =    \n",
    "    let minLength = \n",
    "        labels \n",
    "        |> Matrix.toRowArrays \n",
    "        |> Array.countBy id \n",
    "        |> Array.map snd \n",
    "        |> Array.min\n",
    "    \n",
    "    Seq.zip (features.ToRowArrays()) (labels.ToRowArrays())\n",
    "    |> shuffle\n",
    "    |> Seq.groupBy snd\n",
    "    |> Seq.map (fun (key, grp) -> grp |> Seq.take minLength)\n",
    "    |> Seq.collect id\n",
    "    |> shuffle\n",
    "    |> Seq.map (fun (f,l) -> Seq.append f l)\n",
    "    |> matrix\n",
    "    |> fun mtx -> mtx.[*,..1], mtx.[*,2..]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare:\n",
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generateRandomNonlinearlySeparableDataSample 64 input_dim num_output_classes    \n",
    "||> simpleScatterPlot \"feature 1\" \"feature 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateRandomNonlinearlySeparableDataSample 64 input_dim num_output_classes    \n",
    "||> stratifiedSampling \n",
    "||> simpleScatterPlot \"feature 1\" \"feature 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the parameters necessary for training gathered in one place for your convenience. I've placed them under their own module so we don't mess with the global scope to much, i.e. so you won't have to restart the kernel every time you want to experiment with the parameters, which I very much encourage you to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module NonLinear = \n",
    "    let inputDim, numOutputClasses = 2,2\n",
    "    let learningRate = 0.005\n",
    "    let minibatchSize = 100   \n",
    "    let trainingCycles = 15000\n",
    "    let reportSampleRate = 25\n",
    "    let input = Variable.InputVariable(shape [|inputDim|], dataType, \"Features\")\n",
    "    let label = Variable.InputVariable(shape [|numOutputClasses|], dataType, \"Labels\")\n",
    "    let z = fullyConnectedClassifierNet' input [15;10;5] numOutputClasses (CNTKLib.Sigmoid)\n",
    "    let loss = CNTKLib.CrossEntropyWithSoftmax(Var z, label)\n",
    "    let error = CNTKLib.ClassificationError(Var z, label)\n",
    "    let lrSchedule = new CNTK.TrainingParameterScheduleDouble(learningRate, uint32 CNTK.DataUnit.Minibatch)\n",
    "    let learner = CNTKLib.SGDLearner(z.Parameters() |> ParVec, lrSchedule)\n",
    "    let trainer = CNTK.Trainer.CreateTrainer(z, loss, error, ResizeArray<CNTK.Learner>([learner]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"label label-success\">Extra!</span> Training (with live progress report!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Data logger structure\n",
    "let plotdata = { \n",
    "    BatchSize = ResizeArray<int>()\n",
    "    Loss = ResizeArray<float>()\n",
    "    Error = ResizeArray<float>()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 0..NonLinear.trainingCycles do\n",
    "    let features,labels =\n",
    "        generateRandomNonlinearlySeparableDataSample \n",
    "            NonLinear.minibatchSize (NonLinear.input.Shape.[0]) NonLinear.numOutputClasses\n",
    "        ||> stratifiedSampling\n",
    "        |> fun (x,y) -> matrixToBatch x, matrixToBatch y\n",
    "    \n",
    "    let trainingBatch = [(NonLinear.input, features);(NonLinear.label, labels)] |> dict\n",
    "    let status = NonLinear.trainer.TrainMinibatch(trainingBatch, true, device)\n",
    "    \n",
    "    match (printTrainingProgress NonLinear.trainer i NonLinear.reportSampleRate false) with\n",
    "    | Some (i,loss,eval) ->         \n",
    "        if plotdata.BatchSize.Count > 0 \n",
    "        then plotdata.BatchSize \n",
    "             |> Seq.last \n",
    "             |> (+) NonLinear.reportSampleRate \n",
    "             |> plotdata.BatchSize.Add\n",
    "        else plotdata.BatchSize.Add 1\n",
    "        plotdata.Loss.Add <| loss\n",
    "        plotdata.Error.Add <| eval        \n",
    "    | None -> ()\n",
    "\n",
    "trainingResultPlotSmoothed plotdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting our synchronous training loop is as easy as placing it in an <code>async</code> computational expression. We could stop there and have a very serviceable Async<> object to iterate over with AsyncSeq, but I thought I should pass a few arguments both to make the iterations per cycle to allow for some customization, and to make some additional info available about the general progress of the training.\n",
    "\n",
    "From then on, displaying an updateable label is as simple as returning a string from Async<> and applying IfSharp <code>Display</code> to the resulting AsyncSeq. And you can even return renderable html strings!\n",
    "\n",
    "In fact, here's a simple Bootstrap progress bar to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Bootstrap progress bars for training data reporting\n",
    "/// <remarks> Helper function </remarks>\n",
    "let reportHtml info progress loss error =\n",
    "    let progressBar kind label value =    \n",
    "        System.String.Format(\n",
    "            \"\"\"<div class='progress' style='margin-top:5px; width: 500px'>\n",
    "                   <div class='progress-bar progress-bar-{0} progress-bar-striped' \n",
    "                         role='progressbar' aria-valuenow='{0:f2}'\n",
    "                         aria-valuemin='0' aria-valuemax='100' style='width: {1:f2}%'>\n",
    "                        <span>{1:f2}% ({2})</span>\n",
    "                   </div>\n",
    "                </div>\"\"\", kind, value, label)\n",
    "\n",
    "    [ progressBar \"info\" \"Progress\" progress\n",
    "      progressBar \"warning\" \"Loss\" (loss * 100.)\n",
    "      progressBar \"danger\" \"Error\" (error * 100.) ]\n",
    "    |> List.reduce (+)\n",
    "    |> sprintf \"\"\"<div class='container'><h2>%s</h2>%s</div>\"\"\" info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> IfSharp/Jupyter supports <a href=\"https://getbootstrap.com/\">Bootstrap</a> to some extent out of the box, so don't worry about having to do any extra work referencing css & js libraries. </div>\n",
    "\n",
    "And here's the asynchronous training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open FSharp.Control\n",
    "\n",
    "let trainCycle iterations finalCycle currentCycle htmlReport =\n",
    "    // Our training cycle\n",
    "    for i in 0..iterations do\n",
    "        let features,labels =\n",
    "            generateRandomNonlinearlySeparableDataSample NonLinear.minibatchSize \n",
    "                (NonLinear.input.Shape.[0]) NonLinear.numOutputClasses\n",
    "            ||> stratifiedSampling\n",
    "            |> fun (x,y) -> matrixToBatch x, matrixToBatch y\n",
    "\n",
    "        let trainingBatch = \n",
    "            [(NonLinear.input, features);(NonLinear.label, labels)] |> dict\n",
    "\n",
    "        NonLinear.trainer.TrainMinibatch(trainingBatch, true, device)\n",
    "        |> ignore\n",
    "\n",
    "        (* Let's skip the logging code to keep things shorter *)\n",
    "\n",
    "\n",
    "    // Calculate training info\n",
    "    let lossAverage = NonLinear.trainer.PreviousMinibatchLossAverage()\n",
    "    let evaluationAverage = NonLinear.trainer.PreviousMinibatchEvaluationAverage()\n",
    "    let current = 100. * (float currentCycle + 1.)/(float finalCycle)\n",
    "\n",
    "    async {\n",
    "        // Create report text\n",
    "        let progress = \n",
    "            sprintf \"[%s] %.1f%%\" (\"\".PadLeft(int current,'=').PadRight(100,' ')) current\n",
    "        let info =\n",
    "            sprintf \"Minibatch: %d of %d, Loss: %.4f, Error: %.2f\" \n",
    "                ((currentCycle+1)*iterations) (finalCycle * iterations) lossAverage evaluationAverage;\n",
    "        let progressBar = \n",
    "            if htmlReport then \n",
    "                reportHtml info current lossAverage evaluationAverage\n",
    "            else \n",
    "                sprintf \"<pre>%s\\n %s</pre>\" progress info\n",
    "        \n",
    "        // Send result to AsyncSeq\n",
    "        return progressBar |> Util.Html\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "let totalCycles = 128\n",
    "let iterationsPerCycle = NonLinear.trainingCycles / totalCycles\n",
    "\n",
    "AsyncSeq.initAsync (int64 totalCycles) \n",
    "    (fun i -> trainCycle iterationsPerCycle totalCycles (int i) true)  \n",
    "|> Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let test_minibatch_size = 128\n",
    "let x_test,y_test = generateRandomNonlinearlySeparableDataSample test_minibatch_size input_dim num_output_classes\n",
    "\n",
    "testMinibatch NonLinear.trainer x_test y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSoftmaxOutputHeatmap \"feature 1\" \"feature 2\" [|0. .. 0.1 .. 15.|] NonLinear.z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "F#",
   "language": "fsharp",
   "name": "ifsharp"
  },
  "language": "fsharp",
  "language_info": {
   "codemirror_mode": "",
   "file_extension": ".fs",
   "mimetype": "text/x-fsharp",
   "name": "fsharp",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "4.3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
